---
title: "Modelling"
author: "Phyllis Sun"
date: "2025-03-09"
output: html_document
---
```{r,warning=FALSE}
#' Core SuperLearner package
library(SuperLearner)  

#' Model training and evaluation
library(caret)  

#' Required for different base learners:
library(glmnet)        # Lasso/ElasticNet Regression (SL.glmnet)
library(randomForest)  # Random Forest (SL.randomForest)
library(xgboost)       # Gradient Boosted Trees (SL.xgboost)
library(ranger)        # Fast Random Forest (SL.ranger)
library(nnet)          # Neural Networks (SL.nnet)

```

```{r}
#' Split dataset into training and test sets
#'
#' @description
#' This function partitions the dataset into 80% training and 20% test data 
#' while ensuring class balance for each species.
#'
#' @param data A tibble containing fish data.
#' @return A list containing `train_data` and `test_data`.
split_data <- function(data) {
  set.seed(123)  # Ensure reproducibility
  
  train_list <- list()
  test_list <- list()
  species_list <- unique(data$species)
  
  for (sp in species_list) {
    species_data <- filter(data, species == sp)  # Filter species data
    
    train_index <- createDataPartition(species_data$species, p = 0.8, list = FALSE)
    
    train_list[[sp]] <- species_data[train_index, ]   # Train set for species
    test_list[[sp]]  <- species_data[-train_index, ]  # Test set for species
  }
  
  list(
    train_data = bind_rows(train_list) %>% sample_frac(1),  # Shuffle
    test_data  = bind_rows(test_list) %>% sample_frac(1)    # Shuffle
  )
}
```

```{r}
#' Prepare input (X) and output (Y) datasets
#'
#' @description
#' This function extracts features (X) and binary target labels (Y).
#'
#' @param train_data The training dataset.
#' @param test_data The test dataset.
#' @return A list with `X_train`, `X_test`, `Y_train`, `Y_test`.
prepare_xy <- function(train_data, test_data) {
  X_train <- select(train_data, -species, -fishNum)
  X_test  <- select(test_data, -species, -fishNum)
  
  Y_train <- as.numeric(factor(train_data$species, levels = c("lakeTrout", "smallmouthBass"))) - 1
  Y_test  <- as.numeric(factor(test_data$species, levels = c("lakeTrout", "smallmouthBass"))) - 1
  
  list(X_train = X_train, X_test = X_test, Y_train = Y_train, Y_test = Y_test)
}

```

```{r}
#' Train SuperLearner model
#'
#' @description
#' This function trains a SuperLearner ensemble using multiple base learners.
#'
#' @param X_train The feature matrix for training.
#' @param Y_train The binary target vector for training.
#' @return A trained SuperLearner model.
train_superlearner <- function(X_train, Y_train) {
  learners <- c("SL.glmnet", "SL.randomForest", "SL.xgboost", "SL.nnet", "SL.ranger")
  
  SuperLearner(
    Y = as.numeric(Y_train), 
    X = X_train, 
    family = binomial(),
    SL.library = learners,
    method = "method.NNLS",  # Non-negative least squares
    cvControl = list(V = 5)   # 5-fold cross-validation
  )
}

```

```{r}
# 1. All Biological Features + All Frequencies

# Execute data split
split_results <- split_data(fish_clean)
train_data <- split_results$train_data
test_data  <- split_results$test_data

# Execute X, Y preparation
xy_data <- prepare_xy(train_data, test_data)
X1_train <- xy_data$X_train
X1_test  <- xy_data$X_test
Y1_train <- xy_data$Y_train
Y1_test  <- xy_data$Y_test

# Train SuperLearner Model
SL1 <- train_superlearner(X1_train, Y1_train)
summary(SL1)

# Get direct information from the model
data.frame(
  Algorithm = names(SL1$coef),  # Extract learner names
  Weight = SL1$coef,            # Extract weights
  CV_Risk = SL1$cvRisk           # Extract CV risk for each learner
)
```
Since the result only provide weight for model glmnet, I decide to scale X1_train and X1_test to reducing the impact. 

```{r}
# Standardize X_train and X_test
X1_train_scaled <- as.data.frame(scale(X1_train))
X1_test_scaled <- as.data.frame(scale(X1_test))

SL1_scaled <- train_superlearner(X1_train_scaled, Y1_train)

```

```{r}
data.frame(
  Algorithm = names(SL1_scaled$coef),  # Extract learner names
  Weight = SL1_scaled$coef,            # Extract weights
  CV_Risk = SL1_scaled$cvRisk           # Extract CV risk for each learner
)
```
I decide not to use all biological feature data + all frequency data due to the high correlation between species and biological features which makes glmnet model dominate but our main objective is using frequency response to classify the fish species.

```{r}
# 2. Only Frequencies response

# Execute data split
split_results <- split_data(freq_clean)
train_data <- split_results$train_data
test_data  <- split_results$test_data

# Execute X, Y preparation
xy_data <- prepare_xy(train_data, test_data)
X2_train <- xy_data$X_train
X2_test  <- xy_data$X_test
Y2_train <- xy_data$Y_train
Y2_test  <- xy_data$Y_test

# Train SuperLearner Model
SL2 <- train_superlearner(X2_train, Y2_train)
summary(SL2)

# Get direct information from the model
data.frame(
  Algorithm = names(SL2$coef),  # Extract learner names
  Weight = SL2$coef,            # Extract weights
  CV_Risk = SL2$cvRisk           # Extract CV risk for each learner
)

```

```{r}

 #' Train and evaluate XGBoost model
#'
#' @description
#' This function trains an XGBoost model and computes test accuracy.
#'
#' @param X_train Training feature matrix.
#' @param Y_train Training labels.
#' @param X_test Test feature matrix.
#' @param Y_test Test labels.
#' @return balanced accuracy score,confusion matrix and AUC-ROC scores.

train_xgboost <- function(X_train, Y_train, X_test, Y_test) {
  X_train_mat <- as.matrix(X_train)
  X_test_mat  <- as.matrix(X_test)
  
  xgb_model <- xgboost(data = X_train_mat, label = Y_train, 
                       objective = "binary:logistic", nrounds = 200)
  
  predictions <- predict(xgb_model, newdata = X_test_mat)
  final_predictions <- ifelse(predictions > 0.5, 1, 0)
  
  accuracy <- mean(final_predictions == Y_test) * 100
  message("XGBoost Test Accuracy: ", round(accuracy, 2), "%")
  
  # Compute confusion matrix
  conf_matrix <- confusionMatrix(as.factor(final_predictions), as.factor(Y_test))
  print(conf_matrix)
  
  # Extract sensitivity and specificity
  sensitivity_value <- conf_matrix$byClass["Sensitivity"]
  specificity_value <- conf_matrix$byClass["Specificity"]
  
  # Compute Balanced Accuracy
  balanced_accuracy <- (sensitivity_value + specificity_value) / 2
  message("Balanced Accuracy: ", round(balanced_accuracy, 4))
  
  # Compute AUC-ROC score
  roc_curve <- roc(Y_test, predictions)  # ROC curve
  auc_value <- auc(roc_curve)  # AUC score
  message("AUC-ROC Score: ", round(auc_value, 4))
  
  return(list(
    ConfusionMatrix = conf_matrix,
    BalancedAccuracy = balanced_accuracy,
    AUC = auc_value
  ))
}

# Train XGBoost Model
xgb_model <- train_xgboost(X2_train, Y2_train, X2_test, Y2_test)

```





