---
title: "Modelling"
author: "Phyllis Sun"
date: "2025-03-09"
output: html_document
---
```{r,warning=FALSE}
#' Core SuperLearner package
library(SuperLearner)  

#' Model training and evaluation
library(caret)  

#' Required for different base learners:
library(glmnet)        # Lasso/ElasticNet Regression (SL.glmnet)
library(randomForest)  # Random Forest (SL.randomForest)
library(xgboost)       # Gradient Boosted Trees (SL.xgboost)
library(ranger)        # Fast Random Forest (SL.ranger)
library(nnet)          # Neural Networks (SL.nnet)

```

```{r}
#' Split dataset into training and test sets
#'
#' @description
#' This function partitions the dataset into 80% training and 20% test data 
#' while ensuring class balance for each species.
#'
#' @param data A tibble containing fish data.
#' @return A list containing `train_data` and `test_data`.
split_data <- function(data) {
  set.seed(123)  # Ensure reproducibility
  
  train_list <- list()
  test_list <- list()
  species_list <- unique(data$species)
  
  for (sp in species_list) {
    species_data <- filter(data, species == sp)  # Filter species data
    
    train_index <- createDataPartition(species_data$species, p = 0.8, list = FALSE)
    
    train_list[[sp]] <- species_data[train_index, ]   # Train set for species
    test_list[[sp]]  <- species_data[-train_index, ]  # Test set for species
  }
  
  list(
    train_data = bind_rows(train_list) %>% sample_frac(1),  # Shuffle
    test_data  = bind_rows(test_list) %>% sample_frac(1)    # Shuffle
  )
}
```

```{r}
#' Prepare input (X) and output (Y) datasets
#'
#' @description
#' This function extracts features (X) and binary target labels (Y).
#'
#' @param train_data The training dataset.
#' @param test_data The test dataset.
#' @return A list with `X_train`, `X_test`, `Y_train`, `Y_test`.
prepare_xy <- function(train_data, test_data) {
  X_train <- select(train_data, -species, -fishNum)
  X_test  <- select(test_data, -species, -fishNum)
  
  Y_train <- as.numeric(factor(train_data$species, levels = c("lakeTrout", "smallmouthBass"))) - 1
  Y_test  <- as.numeric(factor(test_data$species, levels = c("lakeTrout", "smallmouthBass"))) - 1
  
  list(X_train = X_train, X_test = X_test, Y_train = Y_train, Y_test = Y_test)
}

# Execute X, Y preparation
xy_data <- prepare_xy(train_data, test_data)
X_train <- xy_data$X_train
X_test  <- xy_data$X_test
Y_train <- xy_data$Y_train
Y_test  <- xy_data$Y_test

```

```{r}
#' Train SuperLearner model
#'
#' @description
#' This function trains a SuperLearner ensemble using multiple base learners.
#'
#' @param X_train The feature matrix for training.
#' @param Y_train The binary target vector for training.
#' @return A trained SuperLearner model.
train_superlearner <- function(X_train, Y_train) {
  learners <- c("SL.glmnet", "SL.randomForest", "SL.xgboost", "SL.nnet", "SL.ranger")
  
  SuperLearner(
    Y = as.numeric(Y_train), 
    X = X_train, 
    family = binomial(),
    SL.library = learners,
    method = "method.NNLS",  # Non-negative least squares
    cvControl = list(V = 5)   # 5-fold cross-validation
  )
}

# Train SuperLearner Model
SL_model <- train_superlearner(X_train, Y_train)
summary(SL_model)

```

```{r}
# 1. All Biological Features + All Frequencies

# Execute data split
split_results <- split_data(fish_clean)
train_data <- split_results$train_data
test_data  <- split_results$test_data

# Execute X, Y preparation
xy_data <- prepare_xy(train_data, test_data)
X1_train <- xy_data$X_train
X1_test  <- xy_data$X_test
Y1_train <- xy_data$Y_train
Y1_test  <- xy_data$Y_test

# Train SuperLearner Model
SL1 <- train_superlearner(X1_train, Y1_train)
summary(SL1)

# Get direct information from the model
data.frame(
  Algorithm = names(SL1$coef),  # Extract learner names
  Weight = SL1$coef,            # Extract weights
  CV_Risk = SL1$cvRisk           # Extract CV risk for each learner
)
```
Since the result only provide weight for model glmnet, I decide to scale X1_train and X1_test to reducing the impact. 

```{r}
# Standardize X_train and X_test
X1_train_scaled <- as.data.frame(scale(X1_train))
X1_test_scaled <- as.data.frame(scale(X1_test))

SL1_scaled <- train_superlearner(X1_train_scaled, Y1_train)

```

```{r}
data.frame(
  Algorithm = names(SL1_scaled$coef),  # Extract learner names
  Weight = SL1_scaled$coef,            # Extract weights
  CV_Risk = SL1_scaled$cvRisk           # Extract CV risk for each learner
)
```
I decide not to use all biological feature data + all frequency data due to the high correlation between species and biological features which makes glmnet model dominate but our main objective is using frequency response to classify the fish species.

```{r}
# 2. Only Frequencies response

# Execute data split
split_results <- split_data(freq_clean)
train_data <- split_results$train_data
test_data  <- split_results$test_data

# Execute X, Y preparation
xy_data <- prepare_xy(train_data, test_data)
X2_train <- xy_data$X_train
X2_test  <- xy_data$X_test
Y2_train <- xy_data$Y_train
Y2_test  <- xy_data$Y_test

# Train SuperLearner Model
SL2 <- train_superlearner(X2_train, Y2_train)
summary(SL2)

# Get direct information from the model
data.frame(
  Algorithm = names(SL2$coef),  # Extract learner names
  Weight = SL2$coef,            # Extract weights
  CV_Risk = SL2$cvRisk           # Extract CV risk for each learner
)

```

```{r}
#' Train and evaluate XGBoost model
#'
#' @description
#' This function trains an XGBoost model and computes test accuracy.
#'
#' @param X_train Training feature matrix.
#' @param Y_train Training labels.
#' @param X_test Test feature matrix.
#' @param Y_test Test labels.
#' @return A trained XGBoost model and accuracy score.
train_xgboost <- function(X_train, Y_train, X_test, Y_test) {
  X_train_mat <- as.matrix(X_train)
  X_test_mat  <- as.matrix(X_test)
  
  xgb_model <- xgboost(data = X_train_mat, label = Y_train, 
                       objective = "binary:logistic", nrounds = 200)
  
  predictions <- predict(xgb_model, newdata = X_test_mat)
  final_predictions <- ifelse(predictions > 0.5, 1, 0)
  
  accuracy <- mean(final_predictions == Y_test) * 100
  message("XGBoost Test Accuracy: ", round(accuracy, 2), "%")
  
  return(xgb_model)
}

# Train XGBoost Model
xgb_model <- train_xgboost(X2_train, Y2_train, X2_test, Y2_test)

```


```{r}
library(randomForest)

# Convert Y2_train to a factor (required for classification)
Y2_train_factor <- as.factor(Y2_train)

# Train a Random Forest model with 200 trees
rf_model <- randomForest(x = X2_train, y = Y2_train_factor, 
                         ntree = 200, importance = TRUE)

# Print model summary
print(rf_model)

# Predict on test data
rf_predictions <- predict(rf_model, newdata = X2_test)

# Compute accuracy
rf_accuracy <- mean(rf_predictions == Y2_test) * 100
print(paste("Random Forest Test Accuracy:", round(rf_accuracy, 2), "%"))

# Confusion Matrix
library(caret)
rf_conf_matrix <- confusionMatrix(as.factor(rf_predictions), as.factor(Y2_test))
print(rf_conf_matrix)

```

```{r}
# Plot feature importance
varImpPlot(rf_model)

```


